experiment:
  name: dinov2-baseline
  name_suffix: rank-16
  seed: 42
  device: auto
  output_dir: ./runs

model:
  name: dinov2_lora
  params:
    model_name: facebook/dinov2-small
    dropout: 0.0
    freeze_backbone: false
    lora:
      enabled: true
      r: 16
      alpha: 32
      dropout: 0.1
      target_modules: ["query", "key", "value", "dense"]

training:
  epochs: 4
  gradient_clip_norm: 0.0
  grad_accumulation: 1
  log_every: 1
  eval_every: 1
  save_every: 1
  precision:
    enabled: true
    dtype: bf16
    grad_scaler: false
  optimizer:
    type: adamw
    lr: 0.0003
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 0.00000001
  scheduler:
    type: cosine
    warmup: 0.1
    min_lr_mult: 0.1
  monitor:
    grad_norm: true
    param_norm: true
    memory: true

data:
  root: /speedy/datasets
  image_size: 224
  batch_size: 256
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  normalization: dinov2
  augment: true
  drop_last: true

logging:
  use_wandb: true
  project: fisher-lora
  entity: null
  run_name: null
  group: null
  tags: ["dinov2", "baseline"]
  notes: null

datasets:
  - name: cifar-100
    val_split: test
    normalization: dinov2
