experiment:
  name: dinov2-bisided-8
  name_suffix: bracket-rout2-rin2-anticommutator
  seed: 42
  device: auto
  output_dir: ./runs

model:
  name: dinov2_bisided
  params:
    model_name: facebook/dinov2-small
    dropout: 0.1
    freeze_backbone: true
    bracket:
      enabled: true
      # r / rank: total low-rank budget; each side uses roughly half (integer split)
      r: 4
      # alpha: scale for the output-side (left) bracket contribution
      alpha: 1.0
      # beta: scale for the input-side (right) bracket contribution
      beta: 1.0
      # lora_alpha: LoRA-style scaling factor (ignored if explicit scaling provided)
      lora_alpha: 8.0
      # dropout: probability applied to skinny projections on both sides (0.0 disables)
      dropout: 0.0
      # mode: commutator (minus) or anticommutator (plus) mixing of brackets
      mode: anticommutator
      # scaling: optional manual scaling override; set to null to derive from lora_alpha
      scaling: null
      # target_modules: suffixes of linear layers to wrap with BracketAdapter
      target_modules: ["query", "key", "value", "dense"]

training:
  epochs: 4
  gradient_clip_norm: 0.0
  grad_accumulation: 1
  log_every: 1
  eval_every: 1
  save_every: 1
  precision:
    enabled: true
    dtype: bf16
    grad_scaler: false
  optimizer:
    type: adamw
    lr: 0.0003
    weight_decay: 0.01
    betas: [0.9, 0.999]
    eps: 0.00000001
  scheduler:
    type: cosine
    warmup: 0.1
    min_lr_mult: 0.1
  monitor:
    grad_norm: true
    param_norm: true
    memory: false

data:
  root: /speedy/datasets
  image_size: 224
  batch_size: 256
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  normalization: dinov2
  augment: true
  drop_last: true

logging:
  use_wandb: true
  project: fisher-lora
  entity: null
  run_name: null
  group: null
  tags: ["dinov2", "bracket", "bisided"]
  notes: null

datasets:
  - name: ImageNet
    val_split: test
    normalization: dinov2
