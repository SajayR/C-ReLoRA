# Configuration for DinoV2 ReLoRA training

# Data configuration
data:
  dataset_path: "/speedy/datasets/ImageNet"
  image_size: 224
  batch_size: 256
  num_workers: 8
  pin_memory: true

# Model configuration
model:
  name: "facebook/dinov2-small"
  num_classes: 1000
  dropout: 0.1

# ReLoRA configuration
relora:
  rank: 2
  alpha: 4
  dropout: 0.0
  merge_scale: null
  target_modules:
    - "query"
    - "key"
    - "value"
    - "dense"
  prune_b_state: true
  merge_frequency: 500
  adapter_learning_rate: 0.001
  adapter_weight_decay: 0.0

# Training configuration
training:
  epochs: 5
  learning_rate: 0.0003
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_clip_norm: 1.0
  save_steps: 1000
  eval_steps: 500
  logging_steps: 1

# Optimizer configuration
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 0.00000001

# Scheduler configuration
scheduler:
  type: "cosine"
  warmup_ratio: 0.1

# Logging and saving
logging:
  use_wandb: true
  project_name: "rerelora"
  run_name: c-relora_rank2
  log_dir: "./logs"

# Checkpointing
checkpointing:
  save_dir: "./checkpoints"
  save_total_limit: 3
  save_best_only: false
  metric_for_best: "val_accuracy"

# Evaluation
evaluation:
  metrics: ["accuracy", "top5_accuracy"]
  eval_on_start: false

# Mixed precision training
mixed_precision:
  enabled: true
  dtype: "bf16"

# Distributed training
distributed:
  enabled: false
  backend: "nccl"

# Reproducibility
seed: 42

# Device
device: "cuda"
